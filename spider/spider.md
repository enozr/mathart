
# 爬虫

### 定义

当我们需要数据的时候，整个互联网都是我们的数据库，某种意义上，我国最著名的爬虫应用就是百度，为了让你输入的关键词匹配万千网页，百度首先要抓取大量的网页数据，抓取数据处理一下并存起来，这个过程就是所谓的爬虫。

今日头条是另一个爬虫的案子，今日头条本身不做新闻，但自动爬取互联网上大量新闻，并按照自己的权重去打分，


  什么是爬虫？爬虫相当于模拟浏览器去访问网站，然后把页面上的信息整理后放自己数据库里，不过这类爬虫虽然广泛强大，但是不很精密，我们做的则是所谓定向爬虫，有针对性地访问一些网站，量身定制，一个字段一个字段地搜集，时刻保持数据准确。

  这几个月的经验而言，做爬虫大致有几个思路:

  根据网址自拟规则，比如某个网址 ```www.a.com/00001.html```, 猜测 ```www.a.com/00002.html``` 也是有的，总结一二，通过程序自动生成一堆网址，最后一一访问。

  但也有可能，这个规律不怎么明显，比如你要爬取一个博客所有的文章，每个文章的url都很难推测，但是间接的，网站的列表页规律简单，```blog.a.com/page1.html```, ```blog.a.com/page2.html```... 每个列表页上一堆超级链接，顺藤摸瓜就是文章页，那你可以通过遍历这个列表页得到所有文章的超级链接，然后爬取这些文章。这种方法是很普遍的爬虫模型，很多爬虫库的模型据此设计。
  
  以上2种，大多是基于解析网页进行爬取的，好比你用你的程序模拟了一个假的浏览器，载入网站代码后打开后，解析并生成网页，然后到对应的地方取得对应的参数后拼起来放到你的数据库里。但也有些网站古道心肠，做了些现成的http接口，你访问就给你整理好的数据，有些还特尼玛全面，遇到这些，我等虫农笑逐颜开。

  真正实战，还需克服许多问题，比如，如果每次访问1000个网站，明显会很快，但带宽肯定过不了，很多连接都超时挂了，漏了数据偷鸡不成十八米，而挨个爬取，网速跟不上，又怎么设计个合理的爬取队列？而你总任务爬百万个链接，速度慢得几天都搞不定，任务结束的时候之前的页面都更新了，这如何解？又或者网站检测到你的ip一天访问千万次的，直接封杀，又咋办...

  细不展开了，总有招可以解这些问题，但大多数网站今日没有那么高明，未必为这些细节提前大动干戈，程序先跑起来就好，能抓老鼠就是好猫。爬取大多网站比我想象容易不少，毕竟，对于网站，明枪易躲暗箭难防，漏洞总是会有的，特别是bat以外的各种网站。